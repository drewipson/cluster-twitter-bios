{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd, pyarrow, ast, emoji, regex, json, csv, os\n",
    "from emosent import get_emoji_sentiment_rank"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning and Pre-Processing\n",
    "## Splitting the Data\n",
    "Handling the data in such a large format is not manageable. First we'll go ahead and split the data into more manageable file sizes. I've written the function below\n",
    "to help me manage large CSV files. The default row_limit prior to splitting is 10,000. Because this is such a large dataset, I've split the data into 50,000 rows per CSV and stored them under `../gda_data/interim/split_files`.\n",
    "\n",
    "It should be noted that splitting this large file does take a while."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def split_file(filehandler: object, delimiter: str =',', row_limit: int =10000, output_name_template: str ='output_%s.csv', output_path: str ='.', keep_headers: bool = True):\n",
    "    \"\"\"\n",
    "    Filehandler object opens file for splitting\n",
    "    Splits file into the number of rows determined by the method argument (default is 10,000 rows). Default delimiter is comma but can be changed by passing a method argument.\n",
    "    Output_name_template is the file naming convention passed with an incrementer number included in the file name. The default output is csv file. The default path argument\n",
    "    is set to the current directory. The keep_headers argument outputs file headers into each new file split and the default value is True.\n",
    "    \"\"\"\n",
    "    reader = csv.reader(filehandler, delimiter=delimiter)\n",
    "    current_piece = 1\n",
    "    current_out_path = os.path.join(\n",
    "         output_path,\n",
    "         output_name_template  % current_piece\n",
    "    )\n",
    "    current_out_writer = csv.writer(open(current_out_path, 'w'), delimiter=delimiter)\n",
    "    current_limit = row_limit\n",
    "    if keep_headers:\n",
    "        headers = next(reader)\n",
    "        current_out_writer.writerow(headers)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i + 1 > current_limit:\n",
    "            current_piece += 1\n",
    "            current_limit = row_limit * current_piece\n",
    "            current_out_path = os.path.join(\n",
    "               output_path,\n",
    "               output_name_template  % current_piece\n",
    "            )\n",
    "            current_out_writer = csv.writer(open(current_out_path, 'w'), delimiter=delimiter)\n",
    "            if keep_headers:\n",
    "                current_out_writer.writerow(headers)\n",
    "        current_out_writer.writerow(row)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Split the data set into files of 50,000 rows.\n",
    "split_file(open('../gda_data/raw/2021-all-Ads-tweets.csv','r'), row_limit=50000, output_path='../gda_data/interim/split_csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Removing Unncessary Data & Changing Data Strings\n",
    "To improve processing time, we'll go ahead and drop some columns we don't need from the csv files to improve memory management and processing. We'll go ahead and store the data under a new\n",
    "directory `./data/clean_data`. We'll also be changing the serialization of the files from .csv to .feather file types for faster processing.\n",
    "\n",
    "The `ast.literal_eval` function changesthe string to be loaded as a dictionary in a pandas dataframe. We'll do this for both the entities and user columns."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columns_to_drop = ['display_text_range', 'geo', 'in_reply_to_status_id', 'scopes', 'possibly_sensitive', 'quoted_status_id', 'quoted_status_id_str', 'truncated', 'quoted_status_permalink', 'filter_level', 'quoted_status', 'contributors', 'coordinates', 'display_text_range','extended_tweet', 'extended_entities', 'matching_rules','extended_tweet', 'is_quote_status', 'place', 'scopes', 'Table Name1', 'F1' , 'id_str', 'quoted_status_id', 'quoted_status_id_str']\n",
    "for file in os.listdir('../gda_data/split_csv'):\n",
    "    df = pd.read_csv(f'../gda_data/split_csv/{file}')\n",
    "    df = df.drop(columns_to_drop, axis=1)\n",
    "    df['entities'] = df['entities'].apply(lambda x: ast.literal_eval(str(x)))\n",
    "    df['user'] = df['user'].apply(lambda x: ast.literal_eval(str(x)))\n",
    "    new_file_name = file.replace('.csv', '.feather')\n",
    "    \n",
    "    df.to_feather(f'../gda_data/processed/feathers/{new_file_name}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dealing with Unique Data -- EMOJIS ðŸ˜Š\n",
    "There are many types of media that are used today in social media. They are able to communicate in a totally unique way beyond with what we do with text. They communicate ideas and feelings in context. We'll go ahead use the below function to identify emojis and add them to a new column 'emojis' in the dataframe before writing them back to file. We'll also build our emoji_tracker dictionary and write that to a .json file as well."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emoji_tracker = {}\n",
    "def find_emojis(text: str):\n",
    "    \"\"\" Takes a string of text as an argument and identifies emojis and returns a record_level_emoji list to be applied on each row in the Pandas dataframe. The count and sentiment of the emoji is added to the emoji_tracker dictionary.\"\"\"\n",
    "    record_level_emoji = []\n",
    "    emojis = regex.findall(r'\\X', text)\n",
    "    for e in emojis:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in e):\n",
    "            record_level_emoji.append(e)\n",
    "            if e in emoji_tracker:\n",
    "                emoji_tracker[e]['count'] += 1\n",
    "            else:\n",
    "                try:\n",
    "                    emoji_tracker[e] = {\n",
    "                        'count': 1,\n",
    "                        'sentiment': get_emoji_sentiment_rank(e)\n",
    "                    }\n",
    "                except:\n",
    "                    emoji_tracker[e] = {\n",
    "                        'count': 1\n",
    "                    }\n",
    "\n",
    "    return record_level_emoji"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for file in os.listdir('data/clean_data'):\n",
    "    df = pd.read_feather(f'data/clean_data/{file}')\n",
    "    df['emojis'] = df['text'].apply(find_emojis)\n",
    "    df.to_feather(f'data/clean_data/{file}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Store Our Data\n",
    "Let's go ahead and store our emoji_tracker dictionary as a `.json` file under `data/descriptions` and write it out to a `.csv`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('data/descriptions/emojis.json', 'w') as fh:\n",
    "    fh.write(json.dumps(emoji_tracker))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emoji_df = pd.DataFrame.from_dict(emoji_tracker, orient='index')\n",
    "emoji_df = pd.concat([emoji_df.drop(['sentiment'], axis=1), emoji_df['sentiment'].apply(pd.Series)] , axis=1)\n",
    "emoji_df = emoji_df.drop([0], axis=1)\n",
    "emoji_df.to_csv('exported_data/emojis.csv', index=True)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}